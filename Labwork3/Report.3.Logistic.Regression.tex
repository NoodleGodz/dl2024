
\documentclass{article}
\title{Labwork 2: Linear Regression}
\author{Nguyen Dang Minh - M23.ICT.008}
\date{\today}


\begin{document}

\maketitle

\section{Introduction}
I hope my teacher didn't try to run this file then

\section{Linear Regression Implementation}

\begin{itemize}
    \item \texttt{load\_csv(file\_path)}: Loads CSV data from \texttt{file\_path}.
    \item \texttt{L(X, y, w1, w0)}: Calculates mean squared error loss.
    \item \texttt{deri\_w1(X, y, w1, w0)}: Computes gradient of loss w.r.t. w1 parameter.
    \item \texttt{deri\_w0(X, y, w1, w0)}: Computes gradient of loss w.r.t. w0 parameter.
    \item \texttt{print\_step(time, w1, w0, loss)}: Prints iteration step, parameter values, and loss.
    \item \texttt{GradientDescent2D(X, y, w1, w0, L, deri\_w1, deri\_w0, lr, stop)}: Performs gradient descent to optimize parameters.
    \item \texttt{LinearRegression(X, y)}: linear regression process, initializes parameters, and prints final regression equation.
\end{itemize}

\section{Different learning rate L}
We use 
\begin{itemize}
    \item X: [1.0, 2.0, 3.0, 4.0, 5.0]
    \item y: [3.0, 5.0, 7.0, 9.0, 11.0]
    \item w1 = 1
    \item w0 = 0
    \item stop = 0.000001
\end{itemize}

\textbf{learning rate = 0 ̣15}
time    w1      w2      L()
0       1.000   0.000   9.000
1       3.100   -0.345  3.121
.....
156     2.004   0.987   0.000
f(x) = 2.004x + 0.987
-> lr just right

\textbf{learning rate = 0 ̣01}
time    w1      w2      L()
0       1.000   0.000   9.000

.....
1479    2.016   0.943   0.000
f(x) = 2.016x + 0.943
-> lr too small

\textbf{learning rate = 0 ̣07}
time    w1      w2      L()
0       1.000   0.000   9.000
1       1.980   0.074   0.486
2       2.190   0.099   0.091

.....
301     2.006   0.979   0.000
302     2.006   0.979   0.000
f(x) = 2.006x + 0.979
-> lr still too small


\section{Conclusion}
well, it works, badly.

\end{document}